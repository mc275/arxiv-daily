import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

# 获取论文代码链接、GitHub 搜索和 arXiv 论文链接的基础 URL
base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"


# 加载配置文件, 输出可以供arxiv.Search使用的搜索表达式
def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''

    # 将config['keywords']['filters']关键词按照arxiv语法进行组合，使用OR逻辑
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = ' OR ' # TODO

        # 单个领域的搜索表达式生成
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        
        # 多个领域的搜索表达式生成
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    

    # 输出可以供arxiv.Search使用的搜索词
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config

# 获取论文作者信息
def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output

# 对论文按照发表日期进行降序排序
def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output
import requests


# 通过 GitHub API 搜索与论文相关的代码仓库，并返回星数最多的仓库链接
def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link


# 搜索 arXiv 上的论文，并获取论文的详细信息，包括标题、作者、链接、代码链接等
def get_daily_papers(topic,query="slam", max_results=2):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """

    # output结构体，content是标准md格式；content_to_web面向网页md的格式。
    content = dict()
    content_to_web = dict()

    # 调用arxiv接口，按照query搜索式进行论文检索，并根据时间排序。
    search_engine = arxiv.Search(
        query = query,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
    )

    # 遍历搜索结果，
    for result in search_engine.results():
        
        # 论文信息提取
        paper_id            = result.get_short_id()
        paper_title         = result.title
        paper_url           = result.entry_id
        code_url            = base_url + paper_id #TODO
        paper_abstract      = result.summary.replace("\n"," ")
        paper_authors       = get_authors(result.authors)
        paper_first_author  = get_authors(result.authors,first_author = True)
        primary_category    = result.primary_category
        publish_time        = result.published.date()
        update_time         = result.updated.date()
        comments            = result.comment

        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_first_author}")

        # 对于有多个版本的同一篇论文，只提取他的短id,忽略版本号(实际就是最终版)
        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = arxiv_url + 'abs/' + paper_key


        try:
            # 尝试获取代码链接，目前只能根据搜索信息获取
            r = requests.get(code_url).json()
            repo_url = None
            if "official" in r and r["official"]:
                repo_url = r["official"]["url"]
            # TODO: 在github里面搜搜看，可能匹配度不高的
            # else:
            #    repo_url = get_code_link(paper_title)
            #    if repo_url is None:
            #        repo_url = get_code_link(paper_key)
                
            # 根据论文搜索结果生成表格
            if repo_url is not None:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|**[link]({})**|\n".format(
                       update_time,paper_title,paper_first_author,paper_key,paper_url,repo_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({}), Code: **[{}]({})**".format(
                       update_time,paper_title,paper_first_author,paper_url,paper_url,repo_url,repo_url)

            else:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|null|\n".format(
                       update_time,paper_title,paper_first_author,paper_key,paper_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({})".format(
                       update_time,paper_title,paper_first_author,paper_url,paper_url)

            # TODO: 加点评论，目前都是空；后面可以考虑让GPT出个摘要
            comments = None
            if comments != None:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += f"\n"

        except Exception as e:
            logging.error(f"exception: {e} with id: {paper_key}")

    data = {topic:content}
    data_web = {topic:content_to_web}
    return data,data_web

# 之前检索的历史论文(json文件)，重新检索其是否有开源代码仓库
def update_paper_links(filename):
    '''
    weekly update paper links in json file
    '''

    # 字符串解析，提取论文的有效信息(日期、标题、作者、arXiv ID 和代码链)
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date,title,authors,arxiv_id,code

    
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

        # 读取输入的json文件数据，不存在就是空
        json_data = m.copy()

        # 遍历所有大类技术领域
        for keywords,v in json_data.items():
            logging.info(f'keywords = {keywords}')

            # 遍历单个领域的所有检索论文
            for paper_id,contents in v.items():

                # 提取单个论文信息
                contents = str(contents)
                update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)

                # 进行重新格式化(实际和上面一样啊)
                contents = "|{}|{}|{}|{}|{}|\n".format(update_time,paper_title,paper_first_author,paper_url,code_url)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')

                # 查找json中代码链接为空的文章
                valid_link = False if '|null|' in contents else True
                if valid_link:
                    continue
                try:
                    # 检索其是否更新了代码仓库
                    code_url = base_url + paper_id #TODO
                    r = requests.get(code_url).json()
                    repo_url = None
                    if "official" in r and r["official"]:
                        repo_url = r["official"]["url"]
                        if repo_url is not None:
                            new_cont = contents.replace('|null|',f'|**[link]({repo_url})**|')
                            logging.info(f'ID = {paper_id}, contents = {new_cont}')
                            json_data[keywords][paper_id] = str(new_cont)

                except Exception as e:
                    logging.error(f"exception: {e} with id: {paper_id}")
        # dump to json file
        with open(filename,"w") as f:
            json.dump(json_data,f)


# 将新检索的论文信息添加到已有json中(json包含已检索的所有论文数据)
def update_json_file(filename,data_dict):
    '''
    daily update json file using data_dict
    '''
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

    json_data = m.copy()

    # 新检索的论文添加到已有json文件中
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename,"w") as f:
        json.dump(json_data,f)

# JSON文件中的论文信息转换为Markdown 格式
def json_to_md(filename,md_filename,
               task = '',
               to_web = False,
               use_title = True,
               use_tc = True,
               show_badge = True,
               use_b2t = True,
               last_day = 30):
    """
    @param filename:        str json文件路径
    @param md_filename:     str md文件路径
    @param to_web:          bool 是否为网页生成 Markdown 文件，
    @param use_title:       bool 是否在 Markdown 文件中显示表头
    @param use_tc:          bool 是否在 Markdown 文件中添加目录
    @param show_badge:      bool 是否在 Markdown 文件中显示徽章（如贡献者、分支、星标等）
    @param use_b2t:         bool 是否在每个主题部分添加 “返回顶部” 链接
    @param last_day         int 最早多少天前的文献会被列出
    @return None
    """

    # 处理 Markdown 文本中的 LaTeX 数学表达式，确保数学表达式前后有适当的空格。
    def pretty_math(s:str) -> str:
        ret = ''
        match = re.search(r"\$.*\$", s)
        if match == None:
            return s
        math_start,math_end = match.span()
        space_trail = space_leading = ''
        if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_trail = ' '
        if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_leading = ' '
        ret += s[:math_start]
        ret += f'{space_trail}${match.group()[1:-1].strip()}${space_leading}'
        ret += s[math_end:]
        return ret

    # 获取当前日期，将日期转换为字符串，并将 "-" 替换为 "."
    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-','.')
    select_year_ago = datetime.date.today() - datetime.timedelta(days=last_day)


    # 读取json
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)


    # md文件不存在就创建，存在则清空内容
    with open(md_filename,"w+") as f:
        pass
    

    # write data into README.md
    with open(md_filename,"a+") as f:
        
        # 根据参数写入md格式定义
        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")
        if use_title == True:
            #f.write(("<p align="center"><h1 align="center"><br><ins>CV-ARXIV-DAILY"
            #         "</ins><br>Automatically Update CV Papers Daily</h1></p>\n"))
            f.write("## Updated on " + DateNow + "\n")
        else:
            f.write("> Updated on " + DateNow + "\n")

        # 注明使用方法和当前功能的说明文件
        f.write("> Usage instructions: [here](./docs/README.md#usage)\n\n")


        # 添加目录
        if use_tc == True:
            f.write("<details>\n")
            f.write("  <summary>Table of Contents</summary>\n")
            f.write("  <ol>\n")
            for keyword in data.keys():
                day_content = data[keyword]
                if not day_content:
                    continue
                kw = keyword.replace(' ','-')
                f.write(f"    <li><a href=#{kw.lower()}>{keyword}</a></li>\n")
            f.write("  </ol>\n")
            f.write("</details>\n\n")

        # 遍历领域
        for keyword in data.keys():
            # 获取对应论文信息
            day_content = data[keyword]

            if not day_content:
                continue
            # the head of each part
            f.write(f"## {keyword}\n\n")

            # 添加表头
            if use_title == True :
                if to_web == False:
                    f.write("|Publish Date|Title|Authors|PDF|Code|\n" + "|---|---|---|---|---|\n")
                else:
                    f.write("| Publish Date | Title | Authors | PDF | Code |\n")
                    f.write("|:---------|:-----------------------|:---------|:------|:------|\n")

            # 对论文按照时间排序
            day_content = sort_papers(day_content)

            # 写入论文信息到md中
            for _,v in day_content.items():
                if v is not None:
                    # f.write(pretty_math(v)) # make latex pretty
                    try:
                        # 提取日期
                        parts = v.split("|")
                        date_str = parts[1].strip()
                        date = datetime.datetime.strptime(date_str, '**%Y-%m-%d**').date() 
                        # 检查日期是否在1年内
                        if date >= select_year_ago:
                            f.write(pretty_math(v))  # make latex pretty
                    except (IndexError, ValueError):
                        continue

            f.write(f"\n")

            # 添加“返回顶部”链接
            if use_b2t:
                top_info = f"#Updated on {DateNow}"
                top_info = top_info.replace(' ','-').replace('.','')
                f.write(f"<p align=right>(<a href={top_info.lower()}>back to top</a>)</p>\n\n")


        # 显示徽章
        if show_badge == True:
            # we don't like long string, break it!
            f.write((f"[contributors-shield]: https://img.shields.io/github/"
                     f"contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[contributors-url]: https://github.com/Vincentqyw/"
                     f"cv-arxiv-daily/graphs/contributors\n"))
            f.write((f"[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/"
                     f"cv-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[forks-url]: https://github.com/Vincentqyw/"
                     f"cv-arxiv-daily/network/members\n"))
            f.write((f"[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/"
                     f"cv-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[stars-url]: https://github.com/Vincentqyw/"
                     f"cv-arxiv-daily/stargazers\n"))
            f.write((f"[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/"
                     f"cv-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[issues-url]: https://github.com/Vincentqyw/"
                     f"cv-arxiv-daily/issues\n\n"))

    logging.info(f"{task} finished")

# 主函数
def demo(**config):

    # 收集每日获取的论文信息，分别存储不同格式的数据。
    data_collector = []
    data_collector_web= []

    # 从配置文件中获取的关键词和最大文章数量
    keywords = config['kv']                                 
    max_results = config['max_results']                     

    # 指示是否更新 README.md、GitPage 和微信公众号文档。
    publish_readme = config['publish_readme']               
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']

    paper_early_days = config['paper_early_days']

    # 是否展示仓库引用的一些信息
    show_badge = config['show_badge']

    # 是否更新论文链接(True表示不增加新检索，只更新已有论文的代码仓库； False表示添加新检索的论文)
    b_update = config['update_paper_links']
    logging.info(f'Update Paper Link = {b_update}')


    # False则不是更新论文链接，而是搜索新论文并添加到文档中
    if config['update_paper_links'] == False:
        logging.info(f"GET daily papers begin")
        for topic, keyword in keywords.items():
            logging.info(f"Keyword: {topic}")
            data, data_web = get_daily_papers(topic, query = keyword,
                                            max_results = max_results)
            data_collector.append(data)
            data_collector_web.append(data_web)
            print("\n")
        logging.info(f"GET daily papers end")

    # 1. README.md中更新论文搜索内容，用于github仓库展示
    if publish_readme:
        # 获取文件路径
        json_file = config['json_readme_path']
        md_file   = config['md_readme_path']

        ## 之前检索的历史论文(json文件)，重新检索其是否有开源代码仓库
        if config['update_paper_links']:
            update_paper_links(json_file)
        # 添加新检索的论文
        else:
            update_json_file(json_file,data_collector)
        
        # 将json文件转换为README.md文件，这个过程应该包含历史检索数据了
        json_to_md(json_file,md_file, task ='Update Readme', \
            show_badge = show_badge, last_day=paper_early_days) 

    # 2. docs/index.md中更新论文搜索内容，用于个人博客网页展示
    if publish_gitpage:
        # 获取文件路径
        json_file = config['json_gitpage_path']
        md_file   = config['md_gitpage_path']


        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file,data_collector)

         # 将json文件转换为docs/index.md文件，这个过程应该包含历史检索数据了
        json_to_md(json_file, md_file, task ='Update GitPage', \
            to_web = True, show_badge = show_badge, \
            use_tc=False, use_b2t=False, last_day=paper_early_days)

    # 3. docs/wechat.md中更新论文搜索内容，用于公众号展示
    if publish_wechat:
        json_file = config['json_wechat_path']
        md_file   = config['md_wechat_path']
        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector_web)
        json_to_md(json_file, md_file, task ='Update Wechat', \
            to_web=False, use_title= False, show_badge = show_badge, last_day=paper_early_days)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path',type=str, default='config.yaml',
                            help='configuration file path')
    parser.add_argument('--update_paper_links', default=False,
                        action="store_true",help='whether to update paper links etc.')
    args = parser.parse_args()
    config = load_config(args.config_path)
    config = {**config, 'update_paper_links':args.update_paper_links}
    demo(**config)
